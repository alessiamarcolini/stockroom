{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alongside Git\n",
    "Stockroom is built to use alongside git. This tutorial will guide you through a typical git workflow that uses stockroom to\n",
    "- Store data\n",
    "- Use that data to train a network in PyTorch\n",
    "- Version the model as we go\n",
    "- Tag the hyper parameters in different experiments\n",
    "\n",
    "For this tutorial, we use a pretrained PyTorch network to classify cats and dogs. We have divided the whole tutorial into 7 stages.\n",
    "1. Setup the repository\n",
    "2. Download some data and store it in stockroom\n",
    "3. Train the network and save the model + hyper parameters\n",
    "4. Fine tune the hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the repository\n",
    "In a typical software development project, we'll have a git repository ready. Let's make that first.\n",
    "\n",
    "#### Initialize git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /home/hhsecond/mypro/stockroom/examples/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize stock\n",
    "We need to initialize stock repository at the same location. A stock initialization is essentially a hangar initialization (if hangar repo doesn't exist at the given location) and creating `head.stock` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hangar Repo initialized at: /home/hhsecond/mypro/stockroom/examples/.hangar\n",
      "Stock file created\n"
     ]
    }
   ],
   "source": [
    "!stock init --name sherin --email a@b.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial git commit\n",
    "Now we need to make the first commit. Remember, we use this notebook for controlling this workflow tutorial. Versioning the notebook might not be a good idea in this case since each checkout will change the status of our notebook which hinder us from moving forward. But in a typical project workflow you require you to version everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "\n",
      "No commits yet\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\n",
      "\t\u001b[31m.gitignore\u001b[m\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\n",
      "\t\u001b[31mhead.stock\u001b[m\n",
      "\t\u001b[31mrequirements.txt\u001b[m\n",
      "\t\u001b[31mwith-git.ipynb\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master (root-commit) bfe72a8] initialized repo\n",
      " 2 files changed, 2 insertions(+)\n",
      " create mode 100644 .gitignore\n",
      " create mode 100644 head.stock\n"
     ]
    }
   ],
   "source": [
    "!echo \"\\ndownloads\" > .gitignore\n",
    "!git add .gitignore head.stock\n",
    "!git commit -m 'initialized repo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download & Store Data\n",
    "For this tutorial, as most of the tutorials, we'll build a fully connected network to predict hand written digits from MNIST dataset.\n",
    "\n",
    "#### Download images\n",
    "We download the data using below utility functions (inspired from https://gist.github.com/goldsborough/6dd52a5e01ed73a642c1e772084bcd03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def report_download_progress(chunk_number, chunk_size, file_size):\n",
    "    if file_size != -1:\n",
    "        percent = min(1, (chunk_number * chunk_size) / file_size)\n",
    "        bar = '#' * int(64 * percent)\n",
    "        sys.stdout.write('\\r0% |{:<64}| {}%'.format(bar, int(percent * 100)))\n",
    "\n",
    "\n",
    "def download(destination_path, url):\n",
    "    if os.path.exists(destination_path):\n",
    "        print('{} already exists, skipping ...'.format(destination_path))\n",
    "    else:\n",
    "        print('Downloading {} ...'.format(url))\n",
    "        urlretrieve(url, destination_path, reporthook=report_download_progress)\n",
    "\n",
    "def unzip(zipped_path):\n",
    "    unzipped_path = os.path.splitext(zipped_path)[0]\n",
    "    if os.path.exists(unzipped_path):\n",
    "        print('{} already exists, skipping ... '.format(unzipped_path))\n",
    "        return\n",
    "    with gzip.open(zipped_path, 'rb') as zipped_file:\n",
    "        with open(unzipped_path, 'wb') as unzipped_file:\n",
    "            unzipped_file.write(zipped_file.read())\n",
    "            print('\\nUnzipped {} ...'.format(zipped_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloads/train-images-idx3-ubyte.gz already exists, skipping ...\n",
      "downloads/train-images-idx3-ubyte already exists, skipping ... \n",
      "downloads/train-labels-idx1-ubyte.gz already exists, skipping ...\n",
      "downloads/train-labels-idx1-ubyte already exists, skipping ... \n",
      "downloads/t10k-images-idx3-ubyte.gz already exists, skipping ...\n",
      "downloads/t10k-images-idx3-ubyte already exists, skipping ... \n",
      "downloads/t10k-labels-idx1-ubyte.gz already exists, skipping ...\n",
      "downloads/t10k-labels-idx1-ubyte already exists, skipping ... \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RESOURCES = [\n",
    "    'train-images-idx3-ubyte.gz',\n",
    "    'train-labels-idx1-ubyte.gz',\n",
    "    't10k-images-idx3-ubyte.gz',\n",
    "    't10k-labels-idx1-ubyte.gz',\n",
    "]\n",
    "\n",
    "path = Path('downloads')\n",
    "path.mkdir(exist_ok=True)\n",
    "\n",
    "for resource in RESOURCES:\n",
    "    destination = os.path.join(str(path), resource)\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/{}'.format(resource)\n",
    "    download(destination, url)\n",
    "    unzip(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store to StockRoom\n",
    "We need hangar columns ready for stockroom to store data there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Arrayset: image\n",
      "Initialized Arrayset: label\n",
      "Commit message:\n",
      "arrayset initialized\n",
      "Commit Successful. Digest: a=28a09ff56d69697bc313561b362200ae94b389d5\n"
     ]
    }
   ],
   "source": [
    "!hangar arrayset create image INT64 784\n",
    "!hangar arrayset create label INT64 1\n",
    "!stock commit -m 'arrayset initialized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = mndata.load_training()\n",
    "tmpimages, tmplabels = mndata.load_testing()\n",
    "images.extend(tmpimages)\n",
    "labels.extend(tmplabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockroom import StockRoom\n",
    "stock = StockRoom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out COMMIT: a=28a09ff56d69697bc313561b362200ae94b389d5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:28<00:00, 2433.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "with stock.optimize(write=True):\n",
    "    for i in tqdm(range(len(images))):\n",
    "        img = np.array(images[i])\n",
    "        label = np.array(labels[i]).reshape(1)\n",
    "        stock.data['image', i] = img\n",
    "        stock.data['label', i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit message:\n",
      "added data\n",
      "Commit Successful. Digest: a=d6b2e5d8bbc397eda5448b3eadc0dc39e14c123e\n"
     ]
    }
   ],
   "source": [
    "!stock commit -m 'added data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network training\n",
    "Let's build a simple fully connected network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from stockroom import StockRoom\n",
    "\n",
    "def train(model, optimizer, criterion):\n",
    "    stock = StockRoom()\n",
    "\n",
    "    with stock.optimize():\n",
    "        for epoch in range(stock.tag['epoch']):\n",
    "            running_loss = 0\n",
    "            trange = tqdm(range(70000))\n",
    "            for i in trange:\n",
    "                optimizer.zero_grad()\n",
    "                sample = torch.from_numpy(stock.data['image', i]).float()\n",
    "                sample /= 255\n",
    "                out = model(sample).unsqueeze(0)\n",
    "                label = torch.from_numpy(stock.data['label', i])\n",
    "                loss = criterion(out, label)\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if i % 1000 == 0 and i != 0:\n",
    "                    trange.set_description(str(running_loss / i))\n",
    "            stock.model['mnist'] = model.state_dict()\n",
    "            stock.commit('added model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (5): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "stock.tag['lr'] = 0.01\n",
    "stock.tag['momentum'] = 0.5\n",
    "stock.tag['epoch'] = 2\n",
    "stock.commit('hyper params')\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [32, 16]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out COMMIT: a=5c291a0b2d946e3bfa359f754837a112df575bd6\n",
      " * Checking out COMMIT: a=5c291a0b2d946e3bfa359f754837a112df575bd6\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=stock.tag['lr'], momentum=stock.tag['momentum'])\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out COMMIT: a=5c291a0b2d946e3bfa359f754837a112df575bd6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70000 [00:00<?, ?it/s]/home/hhsecond/anaconda3/envs/stockroom/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "0.33214209180101045: 100%|██████████| 70000/70000 [01:20<00:00, 869.94it/s]\n",
      "0.20504333917206713: 100%|██████████| 70000/70000 [01:21<00:00, 854.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tuning\n",
    "The loss doesn't go below 0.2 with the hyper parameters we have. Let's try increasing the number of neurons in the inner layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (5): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_sizes = [128, 64]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out COMMIT: a=8b8b7a2f7966acf1c3b5820470fdc34580ef6aaa\n",
      " * Checking out COMMIT: a=8b8b7a2f7966acf1c3b5820470fdc34580ef6aaa\n",
      " * Checking out COMMIT: a=8b8b7a2f7966acf1c3b5820470fdc34580ef6aaa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.22921190452411824: 100%|██████████| 70000/70000 [03:47<00:00, 307.77it/s]\n",
      "0.12486177534811682: 100%|██████████| 70000/70000 [03:25<00:00, 340.91it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=stock.tag['lr'], momentum=stock.tag['momentum'])\n",
    "\n",
    "train(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has enough learning capacity, we can try reducing the learning rate to avoid the jittering of loss across the valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out COMMIT: a=b54ed6f62420c590e2d3206907e239dfa17945f2\n",
      " * Checking out COMMIT: a=b54ed6f62420c590e2d3206907e239dfa17945f2\n",
      " * Checking out COMMIT: a=b54ed6f62420c590e2d3206907e239dfa17945f2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.05775309108767264: 100%|██████████| 70000/70000 [04:33<00:00, 255.85it/s] \n",
      "0.0373574975491017: 100%|██████████| 70000/70000 [04:57<00:00, 235.37it/s]  \n"
     ]
    }
   ],
   "source": [
    "stock.tag['lr'] = 0.003\n",
    "stock.commit('new lr value')\n",
    "optimizer = optim.SGD(model.parameters(), lr=stock.tag['lr'], momentum=stock.tag['momentum'])\n",
    "\n",
    "train(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Great! Now we have a well trained MNIST classifier, the data and the hyperparameters we have used, saved in stockroom. Perhaps, for this tutorial, we haven't used practical training methedologies, like splitting the dataset into validation / test etc. But the idea of the existence of this example is to show how stockroom could be used in a real world scenario. Stockroom is still under active development and we'll have more features such as dataloaders for pytorch, tensorflow etc soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
